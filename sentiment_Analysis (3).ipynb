{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f89d00ef",
        "outputId": "2323c8fe-7715-4e00-a47b-5f8c1c6d6682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY52FzZeNuZH",
        "outputId": "a9ee1c8d-7f2c-4581-c14a-87018285de93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/archive.zip\n",
            "  inflating: /content/drive/MyDrive/IMDB Dataset.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/archive.zip -d /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import pickle\n",
        "\n",
        "# --- 1. Load and Prepare the Dataset ---\n",
        "print(\"--- Loading and Preparing Dataset ---\")\n",
        "try:\n",
        "    df = pd.read_csv('/content/drive/MyDrive/IMDB Dataset.csv')\n",
        "    df.dropna(inplace=True)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'data/IMDB Dataset.csv' not found. Please download it first.\")\n",
        "    exit()\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# --- 2. Define the Preprocessing Function ---\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    negation_words = {'not', 'no', 'nor', \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"don't\", \"doesn't\", \"didn't\"}\n",
        "    stop_words = stop_words - negation_words\n",
        "\n",
        "    text = re.sub(r'<br\\s*/?>', ' ', text)\n",
        "    text = re.sub(r\"[^a-zA-Z]\", \" \", text).lower()\n",
        "    tokens = text.split()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    clean_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return \" \".join(clean_tokens)\n",
        "\n",
        "print(\"Preprocessing text... (This may take a few minutes)\")\n",
        "df['clean_review'] = df['review'].apply(preprocess_text)\n",
        "print(\"Preprocessing complete.\")\n",
        "\n",
        "# --- 3. Train the Sentiment Model ---\n",
        "print(\"\\n--- Training Baseline Sentiment Model ---\")\n",
        "X = df['clean_review']\n",
        "y = df['sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=15000, ngram_range=(1, 2))\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "sentiment_model = LogisticRegression(max_iter=1000)\n",
        "sentiment_model.fit(X_train_vec, y_train)\n",
        "\n",
        "print(\"\\n--- Baseline Model Evaluation Report ---\")\n",
        "y_pred = sentiment_model.predict(X_test_vec)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# --- 4. Save the model for later use ---\n",
        "if not os.path.exists('models'):\n",
        "    os.makedirs('models')\n",
        "with open('models/sentiment_model.pkl', 'wb') as f:\n",
        "    pickle.dump(sentiment_model, f)\n",
        "with open('models/tfidf_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "print(\"\\n✅ Baseline model and vectorizer saved to 'models/' folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x8Rnxrh9Bw1",
        "outputId": "b670136d-ab1b-41c5-f47a-de4b78202b25"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading and Preparing Dataset ---\n",
            "Preprocessing text... (This may take a few minutes)\n",
            "Preprocessing complete.\n",
            "\n",
            "--- Training Baseline Sentiment Model ---\n",
            "\n",
            "--- Baseline Model Evaluation Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.91      0.89      0.90      5000\n",
            "    positive       0.89      0.91      0.90      5000\n",
            "\n",
            "    accuracy                           0.90     10000\n",
            "   macro avg       0.90      0.90      0.90     10000\n",
            "weighted avg       0.90      0.90      0.90     10000\n",
            "\n",
            "\n",
            "✅ Baseline model and vectorizer saved to 'models/' folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ASPECT_KEYWORDS = {\n",
        "    \"acting\": [\"actor\", \"actress\", \"acting\", \"performance\", \"cast\", \"character\"],\n",
        "    \"plot\": [\"plot\", \"story\", \"script\", \"narrative\", \"storyline\", \"ending\"],\n",
        "    \"visuals\": [\"visuals\", \"effects\", \"cgi\", \"cinematography\", \"scenery\"],\n",
        "    \"directing\": [\"directing\", \"director\", \"filmmaker\", \"style\"]\n",
        "}"
      ],
      "metadata": {
        "id": "nTw6CHp69E-P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_aspect_sentiments(review, model, vectorizer):\n",
        "    \"\"\"\n",
        "    Analyzes a review to find sentiment for predefined aspects.\n",
        "    \"\"\"\n",
        "    clean_review = preprocess_text(review)\n",
        "    review_tokens = clean_review.split()\n",
        "    aspect_sentiments = {}\n",
        "\n",
        "    for aspect, keywords in ASPECT_KEYWORDS.items():\n",
        "        for keyword in keywords:\n",
        "            if keyword in review_tokens:\n",
        "                try:\n",
        "                    keyword_index = review_tokens.index(keyword)\n",
        "                    # Create a \"window\" of text around the keyword\n",
        "                    start = max(0, keyword_index - 10)\n",
        "                    end = min(len(review_tokens), keyword_index + 11)\n",
        "                    context_window = \" \".join(review_tokens[start:end])\n",
        "\n",
        "                    # Use our pre-trained model to predict sentiment on this snippet\n",
        "                    vectorized_window = vectorizer.transform([context_window])\n",
        "                    prediction = model.predict(vectorized_window)[0]\n",
        "\n",
        "                    aspect_sentiments[aspect] = prediction\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing aspect '{aspect}': {e}\")\n",
        "\n",
        "    return aspect_sentiments\n",
        "\n",
        "# --- 3. Example Usage ---\n",
        "example_review = \"The acting by the main actress was incredible and she gave a great performance, but the plot was a bit slow and the storyline was not very engaging.\"\n",
        "\n",
        "print(f\"\\n--- ABSA Results for Example Review ---\\n'{example_review}'\")\n",
        "aspects = get_aspect_sentiments(example_review, sentiment_model, vectorizer)\n",
        "print(aspects)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1eAZAjV9VsW",
        "outputId": "8dda2143-7b6b-4a2e-b4ec-f38d065ce7de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ABSA Results for Example Review ---\n",
            "'The acting by the main actress was incredible and she gave a great performance, but the plot was a bit slow and the storyline was not very engaging.'\n",
            "{'acting': 'positive', 'plot': 'positive'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# --- Download NLTK data if needed ---\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# --- Define the Preprocessing Function (Crucial to have it here) ---\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    negation_words = {'not', 'no', 'nor', \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"don't\", \"doesn't\", \"didn't\"}\n",
        "    stop_words = stop_words - negation_words\n",
        "\n",
        "    text = re.sub(r'<br\\s*/?>', ' ', text)\n",
        "    text = re.sub(r\"[^a-zA-Z]\", \" \", text).lower()\n",
        "    tokens = text.split()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    clean_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return \" \".join(clean_tokens)\n",
        "\n",
        "# --- 1. Load your DataFrame and Preprocess it ---\n",
        "try:\n",
        "    # We load the original CSV again to ensure we have the correct data\n",
        "    df = pd.read_csv('/content/drive/MyDrive/IMDB Dataset.csv')\n",
        "    print(\"--- Dataset loaded ---\")\n",
        "\n",
        "    # Run the preprocessing step to create the 'clean_review' column\n",
        "    print(\"Preprocessing text...\")\n",
        "    df['clean_review'] = df['review'].apply(preprocess_text)\n",
        "    print(\"Preprocessing complete.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Please ensure your DataFrame is loaded and preprocessed. Error: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Your Existing LDA Training Code ---\n",
        "print(\"\\n--- Starting Topic Modeling with LDA ---\")\n",
        "\n",
        "count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
        "dtm = count_vectorizer.fit_transform(df['clean_review'].dropna())\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
        "lda.fit(dtm)\n",
        "print(\"LDA model training complete.\")\n",
        "\n",
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(f\"Topic {topic_idx}:\")\n",
        "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "\n",
        "print(\"\\n--- Top Words for Each Discovered Topic ---\")\n",
        "display_topics(lda, count_vectorizer.get_feature_names_out(), 10)\n",
        "\n",
        "\n",
        "# --- 3. Save the Files ---\n",
        "print(\"\\n--- Saving the LDA model and CountVectorizer ---\")\n",
        "\n",
        "if not os.path.exists('models'):\n",
        "    os.makedirs('models')\n",
        "\n",
        "with open('models/lda_model.pkl', 'wb') as f:\n",
        "    pickle.dump(lda, f)\n",
        "\n",
        "with open('models/count_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(count_vectorizer, f)\n",
        "\n",
        "print(\"\\n✅ Success! 'lda_model.pkl' and 'count_vectorizer.pkl' are saved in the 'models' folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_444e3OZ9YRr",
        "outputId": "03824e0e-7ca9-4770-e130-8a7e03482459"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dataset loaded ---\n",
            "Preprocessing text...\n",
            "Preprocessing complete.\n",
            "\n",
            "--- Starting Topic Modeling with LDA ---\n",
            "LDA model training complete.\n",
            "\n",
            "--- Top Words for Each Discovered Topic ---\n",
            "Topic 0:\n",
            "film character life story love people like make way real\n",
            "Topic 1:\n",
            "film role performance actor best play cast version great star\n",
            "Topic 2:\n",
            "kid like music song little time movie disney voice animal\n",
            "Topic 3:\n",
            "series episode tv like zombie time season original fi sci\n",
            "Topic 4:\n",
            "man movie woman father wife like life girl end scene\n",
            "Topic 5:\n",
            "movie great good like time funny really comedy watch think\n",
            "Topic 6:\n",
            "film scene horror time like director make shot plot character\n",
            "Topic 7:\n",
            "film year time child war family life world american old\n",
            "Topic 8:\n",
            "character good story plot action really movie game like little\n",
            "Topic 9:\n",
            "movie bad like good really make acting time thing people\n",
            "\n",
            "--- Saving the LDA model and CountVectorizer ---\n",
            "\n",
            "✅ Success! 'lda_model.pkl' and 'count_vectorizer.pkl' are saved in the 'models' folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y77LmXdCAUgp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}